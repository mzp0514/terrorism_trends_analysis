{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_files_in_directory(directory):\n",
    "    try:\n",
    "        # Iterate over all files and directories in the specified directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                # Construct the file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Remove the file\n",
    "                os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting files in directory: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T14:30:41.441809Z",
     "start_time": "2024-04-21T14:30:41.437457Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw_data/1995-1997/Files (100) (9).zip\n",
      "data/raw_data/1995-1997/Files (100) (5).ZIP\n",
      "data/raw_data/1995-1997/Files (100) (4).ZIP\n",
      "data/raw_data/1995-1997/Files (100) (8).ZIP\n",
      "data/raw_data/1995-1997/Files (100) (3).ZIP\n",
      "data/raw_data/1995-1997/Files (100) (2).ZIP\n",
      "data/raw_data/1995-1997/Files (100) (1).ZIP\n",
      "data/raw_data/1995-1997/Files (100).zip\n",
      "data/raw_data/1995-1997/Files (100) (7).ZIP\n",
      "data/raw_data/1995-1997/Files (100) (6).ZIP\n",
      "data/raw_data/2013/Files (100) (9).ZIP\n",
      "data/raw_data/2013/Files (100) (5).ZIP\n",
      "data/raw_data/2013/Files (100) (4).ZIP\n",
      "data/raw_data/2013/Files (100) (8).ZIP\n",
      "data/raw_data/2013/Files (100) (3).ZIP\n",
      "data/raw_data/2013/Files (100) (2).ZIP\n",
      "data/raw_data/2013/Files (100) (1).ZIP\n",
      "data/raw_data/2013/Files (100).zip\n",
      "data/raw_data/2013/Files (100) (7).ZIP\n",
      "data/raw_data/2013/Files (100) (6).ZIP\n",
      "data/raw_data/2014/Files (100) (9).ZIP\n",
      "data/raw_data/2014/Files (100) (5).ZIP\n",
      "data/raw_data/2014/Files (100) (4).ZIP\n",
      "data/raw_data/2014/Files (100) (8).ZIP\n",
      "data/raw_data/2014/Files (100) (3).ZIP\n",
      "data/raw_data/2014/Files (100) (2).ZIP\n",
      "data/raw_data/2014/Files (100) (1).ZIP\n",
      "data/raw_data/2014/Files (100).zip\n",
      "data/raw_data/2014/Files (100) (7).ZIP\n",
      "data/raw_data/2014/Files (100) (6).ZIP\n",
      "data/raw_data/2015/Files (100) (9).ZIP\n",
      "data/raw_data/2015/Files (100) (5).ZIP\n",
      "data/raw_data/2015/Files (100) (4).ZIP\n",
      "data/raw_data/2015/Files (100) (8).ZIP\n",
      "data/raw_data/2015/Files (100) (3).ZIP\n",
      "data/raw_data/2015/Files (100) (2).ZIP\n",
      "data/raw_data/2015/Files (100) (1).ZIP\n",
      "data/raw_data/2015/Files (100).zip\n",
      "data/raw_data/2015/Files (100) (7).ZIP\n",
      "data/raw_data/2015/Files (100) (6).ZIP\n",
      "data/raw_data/2012/Files (100) (9).ZIP\n",
      "data/raw_data/2012/Files (100) (5).ZIP\n",
      "data/raw_data/2012/Files (100) (4).ZIP\n",
      "data/raw_data/2012/Files (100) (8).ZIP\n",
      "data/raw_data/2012/Files (100) (3).ZIP\n",
      "data/raw_data/2012/Files (100) (2).ZIP\n",
      "data/raw_data/2012/Files (100) (1).ZIP\n",
      "data/raw_data/2012/Files (100).zip\n",
      "data/raw_data/2012/Files (100) (7).ZIP\n",
      "data/raw_data/2012/Files (100) (6).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (9).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (5).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (4).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (8).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (3).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (2).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (1).ZIP\n",
      "data/raw_data/1990-1994/Files (100).zip\n",
      "data/raw_data/1990-1994/Files (100) (7).ZIP\n",
      "data/raw_data/1990-1994/Files (100) (6).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (9).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (5).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (4).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (8).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (3).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (2).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (1).ZIP\n",
      "data/raw_data/1970-1984/Files (100).zip\n",
      "data/raw_data/1970-1984/Files (100) (7).ZIP\n",
      "data/raw_data/1970-1984/Files (100) (6).ZIP\n",
      "data/raw_data/2008/Files (100) (9).ZIP\n",
      "data/raw_data/2008/Files (100) (5).ZIP\n",
      "data/raw_data/2008/Files (100) (4).ZIP\n",
      "data/raw_data/2008/Files (100) (8).zip\n",
      "data/raw_data/2008/Files (100) (3).ZIP\n",
      "data/raw_data/2008/Files (100) (2).ZIP\n",
      "data/raw_data/2008/Files (100) (1).ZIP\n",
      "data/raw_data/2008/Files (100).ZIP\n",
      "data/raw_data/2008/Files (100) (7).ZIP\n",
      "data/raw_data/2008/Files (100) (6).ZIP\n",
      "data/raw_data/2001/Files (100) (9).ZIP\n",
      "data/raw_data/2001/Files (100) (5).ZIP\n",
      "data/raw_data/2001/Files (100) (4).ZIP\n",
      "data/raw_data/2001/Files (100) (8).ZIP\n",
      "data/raw_data/2001/Files (100) (3).ZIP\n",
      "data/raw_data/2001/Files (100) (2).ZIP\n",
      "data/raw_data/2001/Files (100) (1).ZIP\n",
      "data/raw_data/2001/Files (100).zip\n",
      "data/raw_data/2001/Files (100) (7).ZIP\n",
      "data/raw_data/2001/Files (100) (6).ZIP\n",
      "data/raw_data/2006/Files (100) (9).ZIP\n",
      "data/raw_data/2006/Files (100) (5).ZIP\n",
      "data/raw_data/2006/Files (100) (4).ZIP\n",
      "data/raw_data/2006/Files (100) (8).ZIP\n",
      "data/raw_data/2006/Files (100) (3).ZIP\n",
      "data/raw_data/2006/Files (100) (2).ZIP\n",
      "data/raw_data/2006/Files (100) (1).ZIP\n",
      "data/raw_data/2006/Files (100).zip\n",
      "data/raw_data/2006/Files (100) (7).ZIP\n",
      "data/raw_data/2006/Files (100) (6).ZIP\n",
      "data/raw_data/2007/Files (100) (9).ZIP\n",
      "data/raw_data/2007/Files (100) (5).zip\n",
      "data/raw_data/2007/Files (100) (4).ZIP\n",
      "data/raw_data/2007/Files (100) (8).ZIP\n",
      "data/raw_data/2007/Files (100) (3).ZIP\n",
      "data/raw_data/2007/Files (100) (2).ZIP\n",
      "data/raw_data/2007/Files (100) (1).ZIP\n",
      "data/raw_data/2007/Files (100).zip\n",
      "data/raw_data/2007/Files (100) (7).ZIP\n",
      "data/raw_data/2007/Files (100) (6).ZIP\n",
      "data/raw_data/2009/Files (100) (9).ZIP\n",
      "data/raw_data/2009/Files (100) (5).ZIP\n",
      "data/raw_data/2009/Files (100) (4).ZIP\n",
      "data/raw_data/2009/Files (100) (8).ZIP\n",
      "data/raw_data/2009/Files (100) (3).ZIP\n",
      "data/raw_data/2009/Files (100) (2).ZIP\n",
      "data/raw_data/2009/Files (100) (1).ZIP\n",
      "data/raw_data/2009/Files (100).zip\n",
      "data/raw_data/2009/Files (100) (7).ZIP\n",
      "data/raw_data/2009/Files (100) (6).ZIP\n",
      "data/raw_data/2017/Files (100) (9).ZIP\n",
      "data/raw_data/2017/Files (100) (5).ZIP\n",
      "data/raw_data/2017/Files (100) (4).ZIP\n",
      "data/raw_data/2017/Files (100) (8).ZIP\n",
      "data/raw_data/2017/Files (100) (3).ZIP\n",
      "data/raw_data/2017/Files (100) (2).ZIP\n",
      "data/raw_data/2017/Files (100) (1).zip\n",
      "data/raw_data/2017/Files (100).ZIP\n",
      "data/raw_data/2017/Files (100) (7).ZIP\n",
      "data/raw_data/2017/Files (100) (6).ZIP\n",
      "data/raw_data/2010/Files (100) (9).ZIP\n",
      "data/raw_data/2010/Files (100) (5).ZIP\n",
      "data/raw_data/2010/Files (100) (4).ZIP\n",
      "data/raw_data/2010/Files (100) (8).ZIP\n",
      "data/raw_data/2010/Files (100) (3).ZIP\n",
      "data/raw_data/2010/Files (100) (2).ZIP\n",
      "data/raw_data/2010/Files (100) (1).ZIP\n",
      "data/raw_data/2010/Files (100).zip\n",
      "data/raw_data/2010/Files (100) (7).ZIP\n",
      "data/raw_data/2010/Files (100) (6).ZIP\n",
      "data/raw_data/2019/Files (100) (9).ZIP\n",
      "data/raw_data/2019/Files (100) (5).ZIP\n",
      "data/raw_data/2019/Files (100) (4).ZIP\n",
      "data/raw_data/2019/Files (100) (8).ZIP\n",
      "data/raw_data/2019/Files (100) (3).ZIP\n",
      "data/raw_data/2019/Files (100) (2).ZIP\n",
      "data/raw_data/2019/Files (100) (1).ZIP\n",
      "data/raw_data/2019/Files (100).zip\n",
      "data/raw_data/2019/Files (100) (7).ZIP\n",
      "data/raw_data/2019/Files (100) (6).ZIP\n",
      "data/raw_data/2020/Files (100) (9).ZIP\n",
      "data/raw_data/2020/Files (100) (5).ZIP\n",
      "data/raw_data/2020/Files (100) (4).ZIP\n",
      "data/raw_data/2020/Files (100) (8).ZIP\n",
      "data/raw_data/2020/Files (100) (3).ZIP\n",
      "data/raw_data/2020/Files (100) (2).ZIP\n",
      "data/raw_data/2020/Files (100) (1).ZIP\n",
      "data/raw_data/2020/Files (100).zip\n",
      "data/raw_data/2020/Files (100) (7).ZIP\n",
      "data/raw_data/2020/Files (100) (6).ZIP\n",
      "data/raw_data/2018/Files (100) (9).zip\n",
      "data/raw_data/2018/Files (100) (5).zip\n",
      "data/raw_data/2018/Files (100) (4).ZIP\n",
      "data/raw_data/2018/Files (100) (8).ZIP\n",
      "data/raw_data/2018/Files (100) (3).ZIP\n",
      "data/raw_data/2018/Files (100) (2).ZIP\n",
      "data/raw_data/2018/Files (100) (1).ZIP\n",
      "data/raw_data/2018/Files (100).zip\n",
      "data/raw_data/2018/Files (100) (7).ZIP\n",
      "data/raw_data/2018/Files (100) (6).ZIP\n",
      "data/raw_data/2011/Files (100) (9).ZIP\n",
      "data/raw_data/2011/Files (100) (5).zip\n",
      "data/raw_data/2011/Files (100) (4).ZIP\n",
      "data/raw_data/2011/Files (100) (8).ZIP\n",
      "data/raw_data/2011/Files (100) (3).ZIP\n",
      "data/raw_data/2011/Files (100) (2).ZIP\n",
      "data/raw_data/2011/Files (100) (1).ZIP\n",
      "data/raw_data/2011/Files (100).zip\n",
      "data/raw_data/2011/Files (100) (7).ZIP\n",
      "data/raw_data/2011/Files (100) (6).ZIP\n",
      "data/raw_data/2016/Files (100) (9).ZIP\n",
      "data/raw_data/2016/Files (100) (5).ZIP\n",
      "data/raw_data/2016/Files (100) (4).ZIP\n",
      "data/raw_data/2016/Files (100) (8).ZIP\n",
      "data/raw_data/2016/Files (100) (3).ZIP\n",
      "data/raw_data/2016/Files (100) (2).ZIP\n",
      "data/raw_data/2016/Files (100) (1).ZIP\n",
      "data/raw_data/2016/Files (100).ZIP\n",
      "data/raw_data/2016/Files (100) (7).ZIP\n",
      "data/raw_data/2016/Files (100) (6).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (9).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (5).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (4).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (8).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (3).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (2).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (1).ZIP\n",
      "data/raw_data/1998-2000/Files (100).zip\n",
      "data/raw_data/1998-2000/Files (100) (7).ZIP\n",
      "data/raw_data/1998-2000/Files (100) (6).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (9).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (5).zip\n",
      "data/raw_data/1985-1989/Files (100) (4).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (8).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (3).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (2).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (1).ZIP\n",
      "data/raw_data/1985-1989/Files (100).zip\n",
      "data/raw_data/1985-1989/Files (100) (7).ZIP\n",
      "data/raw_data/1985-1989/Files (100) (6).ZIP\n",
      "data/raw_data/2005/Files (100) (9).ZIP\n",
      "data/raw_data/2005/Files (100) (5).ZIP\n",
      "data/raw_data/2005/Files (100) (4).zip\n",
      "data/raw_data/2005/Files (100) (8).ZIP\n",
      "data/raw_data/2005/Files (100) (3).ZIP\n",
      "data/raw_data/2005/Files (100) (2).ZIP\n",
      "data/raw_data/2005/Files (100) (1).ZIP\n",
      "data/raw_data/2005/Files (100).zip\n",
      "data/raw_data/2005/Files (100) (7).ZIP\n",
      "data/raw_data/2005/Files (100) (6).ZIP\n",
      "data/raw_data/2002/Files (100) (9).ZIP\n",
      "data/raw_data/2002/Files (100) (5).ZIP\n",
      "data/raw_data/2002/Files (100) (4).ZIP\n",
      "data/raw_data/2002/Files (100) (8).ZIP\n",
      "data/raw_data/2002/Files (100) (3).ZIP\n",
      "data/raw_data/2002/Files (100) (2).ZIP\n",
      "data/raw_data/2002/Files (100) (1).ZIP\n",
      "data/raw_data/2002/Files (100).zip\n",
      "data/raw_data/2002/Files (100) (7).ZIP\n",
      "data/raw_data/2002/Files (100) (6).ZIP\n",
      "data/raw_data/2003/Files (100) (9).ZIP\n",
      "data/raw_data/2003/Files (100) (5).ZIP\n",
      "data/raw_data/2003/Files (100) (4).ZIP\n",
      "data/raw_data/2003/Files (100) (8).ZIP\n",
      "data/raw_data/2003/Files (100) (3).ZIP\n",
      "data/raw_data/2003/Files (100) (2).ZIP\n",
      "data/raw_data/2003/Files (100) (1).ZIP\n",
      "data/raw_data/2003/Files (100).ZIP\n",
      "data/raw_data/2003/Files (100) (7).ZIP\n",
      "data/raw_data/2003/Files (100) (6).ZIP\n",
      "data/raw_data/2004/Files (100) (9).ZIP\n",
      "data/raw_data/2004/Files (100) (5).ZIP\n",
      "data/raw_data/2004/Files (100) (4).ZIP\n",
      "data/raw_data/2004/Files (100) (8).ZIP\n",
      "data/raw_data/2004/Files (100) (3).ZIP\n",
      "data/raw_data/2004/Files (100) (2).ZIP\n",
      "data/raw_data/2004/Files (100) (1).ZIP\n",
      "data/raw_data/2004/Files (100).zip\n",
      "data/raw_data/2004/Files (100) (7).ZIP\n",
      "data/raw_data/2004/Files (100) (6).ZIP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import uuid\n",
    "\n",
    "def extract_zips_and_move(src_dir, dest_dir):\n",
    "    # Iterate over all files and directories in the source directory\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            # Check if the file is a zip file\n",
    "            if file.endswith('.zip') or file.endswith('.ZIP'):\n",
    "                zip_path = os.path.join(root, file)\n",
    "                \n",
    "                # Create a temporary directory to extract the files\n",
    "                temp_dir = os.path.join(dest_dir, 'temp_extract')\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                \n",
    "                try:\n",
    "                    # Extract the zip file\n",
    "                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                        print(zip_path)\n",
    "                        zip_ref.extractall(temp_dir)\n",
    "                    \n",
    "                    for extracted_file in os.listdir(temp_dir):\n",
    "                        extracted_file_path = os.path.join(temp_dir, extracted_file)\n",
    "                        # Check if the file already exists in the destination directory\n",
    "                        dest_file_path = os.path.join(dest_dir, extracted_file)\n",
    "                        if os.path.exists(dest_file_path):\n",
    "                           continue\n",
    "                        \n",
    "                        shutil.move(extracted_file_path, dest_file_path)\n",
    "                    \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting {zip_path}: {e}\")\n",
    "                \n",
    "                finally:\n",
    "                    # Clean up the temporary directory\n",
    "                    shutil.rmtree(temp_dir)\n",
    "\n",
    "source_directory = 'data/raw_data'\n",
    "destination_directory = 'data/rtf_data'\n",
    "\n",
    "delete_files_in_directory(destination_directory)\n",
    "extract_zips_and_move(source_directory, destination_directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T14:30:56.751762Z",
     "start_time": "2024-04-21T14:30:49.132040Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "def extract_text_from_rtf_files(input_directory, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Iterate over all files and directories in the input directory\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.rtf'):\n",
    "                input_file_path = os.path.join(root, file_name)\n",
    "                output_file_path = os.path.join(output_directory, os.path.splitext(file_name)[0] + \".txt\")\n",
    "                try:\n",
    "                    # Extract text from the RTF file\n",
    "                    with open(input_file_path, 'r', encoding='utf-8', errors='ignore') as rtf_file:\n",
    "                        rtf_text = rtf_file.read()\n",
    "                        text = rtf_to_text(rtf_text).strip().replace('\\xa0', ' ')\n",
    "                    \n",
    "                    # Write the extracted text to a TXT file\n",
    "                    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                        output_file.write(text)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting text from {input_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_directory = 'data/rtf_data'\n",
    "output_directory = 'data/txt_data'\n",
    "\n",
    "delete_files_in_directory(output_directory)\n",
    "extract_text_from_rtf_files(input_directory, output_directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T14:37:04.978694Z",
     "start_time": "2024-04-21T14:30:59.899457Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the new york times', 'university wire', 'st. louis post-dispatch ', 'pittsburgh post-gazette', 'the charleston gazette-mail', 'the christian science monitor', 'the philadelphia inquirer', 'the deseret news', 'telegraph herald ', 'usa today', 'tampa bay times', 'the atlanta journal-constitution', 'daily news ', 'chicago daily herald', 'dayton daily news ', 'the tampa tribune ', 'the bismarck tribune', 'richmond times dispatch ', 'new york sun ', 'star tribune ', 'the columbian ', 'the salt lake tribune', 'the spokesman-review', 'new york times abstracts', 'the philadelphia daily news', 'bangor daily news ', 'pittsburgh tribune review', 'the pantagraph', 'wisconsin state journal', 'lnp ', 'the forward', 'business insurance', 'the capital ', 'lancaster  newspapers', 'portland press herald', 'the will ', 'the santa fe new mexican', 'contentengine think tank newswire english', 'ce noticias financieras ', 'american banker', 'anchorage daily news', 'air force times - newspaper edition', 'the chronicle of higher education', 'new york observer', 'the hill', 'usa today online', \"discover america's story\", 'federal times - newspaper edition', 'tribune-review', 'the wyoming tribune-eagle ', 'lincoln journal star ', 'the sentinel', 'asean tribune', 'metro vaartha', 'thehill.com', 'the daily record ', 'the capital times ', 'the hollywood reporter', 'the brunswick news ', 'long island business news ', 'advertising age', 'congressnow', 'the daily record of rochester ', \"crain's chicago business\", 'the times-tribune ', \"crain's new york business\", 'the baltimore sun', 'maryland gazette', 'the hartford courant', 'the morning call', 'the anniston star ', 'wall street journal abstracts', 'idaho falls post register', 'east valley tribune ', 'the lima news ', 'st. joseph news-press ', 'new orleans citybusiness ', 'watertown daily times ', 'los angeles times', 'the chronicle of philanthropy', \"crain's detroit business\", 'the press democrat', 'yellow sheet report', 'journal record legislative report ', 'the day ', 'stars and stripes', 'herald-times ', 'the dominion post ', 'odessa american ', 'arizona capitol times', \"crain's cleveland business\", \"the citizens' voice \", 'post-bulletin ', 'the beaumont enterprise ', 'messenger-inquirer ', 'enid news & eagle ', 'northeast mississippi daily journal ', 'the indianapolis business journal', 'the janesville gazette ', 'aiken standard ', 'colorado springs business journal ', 'the bakersfield californian', 'standard-speaker ', 'the frederick news-post ', 'the taos news', 'high point enterprise ', 'the monitor ', 'the albany herald ', 'the brownsville herald ', 'valley morning star ', 'national mirror', 'the keene sentinel ', 'the leader-telegram ', 'government technology', 'education week', 'republican & herald ', 'daily inter lake', 'daily gazette ', 'the wilson daily times ', 'finance & commerce ', 'military supplements', 'the idaho business review ', 'the daily citizen ', 'daily journal of commerce ', 'the wenatchee world ', 'norwalk reflector', 'the virgin islands daily news', 'the natchez democrat ', 'the bemidji pioneer ', 'walla walla union-bulletin ', 'insidesources.com', 'the ironton tribune', 'tire business', 'midland daily news', 'corsicana daily sun ', 'albert lea tribune ', 'austin daily herald', 'jacksonville journal-courier ', 'the valley news-dispatch ', 'lake county news-sun', 'sanford herald ', 'the mountaineer ', 'the dispatch', 'mississippi business journal ', 'junction city daily union', 'morris daily herald ', 'richmond county daily journal ', 'rubber & plastics news', 'news-topic ', 'the daily globe ', 'corning observer ', 'stateline.org', 'the elkhart truth ', 'the lebanon democrat ', 'the robesonian ', 'howard county times', 'the times leader', 'the tribune-star', 'the creston news advertiser ', 'crossville chronicle ', 'newton daily news', 'the daily star-journal', 'the virginia gazette', 'colusa county sun-herald ', 'the newberry observer', 'effingham daily news ', 'fergus falls daily journal', 'skagit valley herald ', 'the beacon-news', 'the observer ', 'sampson independent ', 'the laurinburg exchange ', 'american medical news', 'the union-recorder', 'the bond buyer', 'the mecklenburg times ', 'baker city herald', 'the derry news ', 'the easley progress ', 'the manhattan mercury ', 'the news-gazette ', 'gallipolis daily tribune', 'henderson  daily dispatch', 'hungry horse news', 'the courier-news', 'the daily sentinel', 'the telegraph ', 'citybusiness north shore report ', 'columbia basin herald', 'point pleasant register ', 'the decatur daily ', 'appeal-democrat', 'greensburg daily news', 'central penn business journal ', 'bladen journal', 'dowagiac daily news', 'kilgore news herald ', 'the chronicle ', 'the express-star ', 'the whitefish pilot ', 'the world', 'tyler morning telegraph ', 'the daily news of newburyport', 'lehigh valley business', 'bridgetower legal & public notice', 'towson times', 'mail tribune', 'pharos-tribune', 'the news-herald', 'abilene reflector-chronicle', 'allied news', 'athens daily review ', 'niles daily star', 'the pine journal ', 'lodi news-sentinel', 'north american oil & gas monitor', 'clark fork valley press & mineral independent', 'edwardsburg argus', 'glenn county transcript', 'lake county leader', 'washington times-herald ', 'the times-news', 'sambad english', 'kent reporter ', 'albany democrat-herald', 'eastern new mexico news', 'moscow-pullman daily news', 'star beacon', 'the herald banner', 'the sentinel echo', 'the tifton gazette', 'the times', 'st. petersburg times ', 'christian science monitor ', 'charleston gazette', 'charleston daily mail', 'the atlanta journal and constitution', 'philadelphia daily news', 'spokesman review', 'intelligencer journal', 'sunday news', 'wyoming tribune-eagle', 'lancaster new era', 'rtt news', 'the munday courier', 'air force times', 'ips', 'telegraph herald', 'philadelphia inquirer', 'federal times', 'investnews ', 'santa fe new mexican', 'the hancock news', 'the billings county pioneer', \"hart county's newspaper news-herald\", 'the news-examiner', 'heritage florida jewish news', 'hays free press', 'the horry independent', 'lassen county times', 'the jena times', 'the oskaloosa independent', 'the sebree banner', 'noticiasfinancieras', 'the coquille valley sentinel', 'the quincy valley post-register', 'turtle mountain star', 'stillwater journal', 'the foothills sun-gazette', 'the brooke county review', 'the raton range', 'mattawa area news', 'business wire latin america', 'the journal', 'the tuskegee news', 'post-gazette', 'basin republican rustler', 'the northwood anchor', 'hollywoodreporter.com', 'de baca county news', 'the lake metigoshe mirror', 'the southerner', 'the lovell chronicle', 'the othello outlook', 'the issaquah press', 'spring hope enterprise', 'the meriwether vindicator', 'the florala news', 'arizona range news', 'blackfoot valley dispatch', 'the beaumont enterprise', 'decision times', 'vineyard gazette', 'the pendleton record', 'the catalina islander', 'americanbanker.com', 'the trinidad times independent', 'the north weld herald', 'mineral county independent-news', 'courier', 'morris daily herald', 'the keene sentinel', 'alaska dispatch news', 'metro vartha', 'future of freedom foundation', 'cape gazette', 'daily record', 'economic thinking', 'cato institute', 'the foundation for research on equal opportunity', 'long island business news', 'macdonald-laurier institute for public policy', 'american institute for economic research', 'civitas', 'frontier centre for public policy', 'the libertarian institute', 'henderson', 'journal record legislative report', \"young america's foundation\", 'idaho business review', 'journal record legislative report', 'r street institute', 'freedom works foundation']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def delete_within_parentheses_and_after_commas(string):\n",
    "    # Delete text within parentheses\n",
    "    string = re.sub(r'\\([^)]*\\)', '', string)\n",
    "    # Delete text after commas\n",
    "    string = re.sub(r',.*', '', string)\n",
    "    return string\n",
    "\n",
    "publisher_file = \"publishers\"\n",
    "\n",
    "with open(publisher_file, 'r', encoding='utf-8') as file:\n",
    "    # Read all lines from the file and store them in the array\n",
    "    publishers = file.readlines()\n",
    "\n",
    "publishers = [delete_within_parentheses_and_after_commas(s.strip().lower()) for s in publishers if len(s) > 0 and s[0].isalpha()]\n",
    "print(publishers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T14:56:20.850248Z",
     "start_time": "2024-04-21T14:56:20.842624Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_article_info(title, article_text):\n",
    "    publisher = \"PUBLISHER NOT FOUND\"\n",
    "    for i in range(len(publishers)):\n",
    "        if article_text.lower().find(publishers[i]) != -1:\n",
    "            publisher = publishers[i]\n",
    "            break\n",
    "            \n",
    "    if publisher == \"PUBLISHER NOT FOUND\":\n",
    "        print(title)\n",
    "        print(article_text.split('\\n')[1])\n",
    "\n",
    "    try:\n",
    "        match = re.search(r'(?i)\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?) (0?[1-9]|[12][0-9]|3[01]), [0-9]+', article_text)\n",
    "        date = match.group()\n",
    "\n",
    "        date = datetime.strptime(date, '%B %d, %Y')\n",
    "        day = date.day\n",
    "        month = date.month\n",
    "        year = date.year\n",
    "    except AttributeError:\n",
    "        year = \"0000\"\n",
    "        month = \"00\"\n",
    "        day = \"00\"   \n",
    "    except ValueError:\n",
    "        year = \"0000\"\n",
    "        month = \"00\"\n",
    "        day = \"00\"\n",
    "\n",
    "    full_text = \"\\n\".join(article_text.split(\"\\n\")[article_text.split(\"\\n\").index(\"Body\")+1:article_text.split('\\n').index(\"End of Document\")]).strip().strip('\\n')\n",
    "\n",
    "    article_info = {\"title\": title, \n",
    "                    \"publisher\": publisher,\n",
    "                    \"year\": year, \n",
    "                    \"month\": month, \n",
    "                    \"day\": day, \n",
    "                    \"full text\": full_text}\n",
    "    \n",
    "    return article_info\n",
    "\n",
    "def process_txt_files(input_directory, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate over all files and directories in the input directory\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.txt'):\n",
    "                input_file_path = os.path.join(root, file_name)\n",
    "                output_file_path = os.path.join(output_directory, os.path.splitext(file_name)[0] + \".json\")\n",
    "                try:\n",
    "                    # Read the text file\n",
    "                    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                        article_text = file.read()\n",
    "                    \n",
    "                    # Extract article information\n",
    "                    if \"Body\" in article_text:\n",
    "                        article_info = extract_article_info(os.path.splitext(file_name)[0], article_text)\n",
    "                    else:\n",
    "                         continue\n",
    "\n",
    "                    # Write the extracted information to a JSON file\n",
    "                    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                        json.dump(article_info, output_file, ensure_ascii=False, indent=4)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_file_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory = 'data/txt_data'\n",
    "output_directory = 'data/json_data'\n",
    "\n",
    "delete_files_in_directory(output_directory)\n",
    "process_txt_files(input_directory, output_directory)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T14:56:35.335090Z",
     "start_time": "2024-04-21T14:56:24.982409Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T14:56:52.202588Z",
     "start_time": "2024-04-21T14:56:48.047986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def parse_json_files_to_csv(input_directory, output_csv):\n",
    "    # Initialize a list to store all rows\n",
    "    all_rows = []\n",
    "\n",
    "    # Iterate over all files and directories in the input directory\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.json'):\n",
    "                input_file_path = os.path.join(root, file_name)\n",
    "                try:\n",
    "                    # Read the JSON file\n",
    "                    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                        data = json.load(file)\n",
    "                    \n",
    "                    # Append the data from the JSON file to the list of rows\n",
    "                    all_rows.append(data)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {input_file_path}: {e}\")\n",
    "\n",
    "    print(len(all_rows))\n",
    "    # Get the keys (column names) from the first JSON object\n",
    "    if all_rows:\n",
    "        fieldnames = list(all_rows[0].keys())\n",
    "    else:\n",
    "        print(\"No JSON files found.\")\n",
    "        return\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()  # Write the header row\n",
    "        for row in all_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage\n",
    "input_directory = 'data/json_data'\n",
    "output_csv = 'data/output.csv'\n",
    "\n",
    "parse_json_files_to_csv(input_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
